#!/bin/bash
#
#SBATCH --job-name=week5
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --time=1:00:00
#SBATCH --mem=8GB
#SBATCH --output=Week5_md.out
# Generated by CHARMM-GUI (http://www.charmm-gui.org) v3.7
#
# This folder contains GROMACS formatted CHARMM36 force fields, a pre-optimized PDB structure, and GROMACS inputs.
# All input files were optimized for GROMACS 2019.2 or above, so lower version of GROMACS can cause some errors.
# We adopted the Verlet cut-off scheme for all minimization, equilibration, and production steps because it is 
# faster and more accurate than the group scheme. If you have a trouble with a performance of Verlet scheme while 
# running parallelized simulation, you should check if you are using appropriate command line.
# For MPI parallelizing, we recommand following command:
# mpirun -np $NUM_CPU mpirun gmx_mpi mdrun -ntomp 1



gmx_mpi grompp -f adp_T300.mdp -c adp.gro -p adp.top -o adp_md.tpr
gmx_mpi mdrun -v -deffnm adp_md
